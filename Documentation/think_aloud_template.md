# Think-Aloud Testing Protocol

## Project: Student Portal Platform
**Design Thinking Deliverable - Stage 5: Test**

---

## Purpose
Think-aloud testing is a usability testing method where participants verbalize their thoughts, feelings, and actions while interacting with the Student Portal. This document provides templates and records for conducting and analyzing think-aloud sessions.

---

## Testing Protocol

### Before the Session

#### Participant Preparation
1. **Introduction**: Explain the purpose of the test
2. **Consent**: Obtain permission for recording/data collection
3. **Instructions**: Guide them on how to "think aloud"
4. **Practice Task**: Allow them to practice the technique

#### Example Script
```
"Thank you for participating in this usability test. We're testing the Student Portal, 
not you. There are no right or wrong answers. 

Please think out loud as you use the portal - tell us what you're looking at, what 
you're trying to do, what you're thinking, and how you're feeling. If you get stuck, 
that's okay - just tell us what's confusing you.

We'll be recording your session for analysis. Do you have any questions before we begin?"
```

### During the Session

#### Facilitator Guidelines
- ✅ DO: Encourage continuous verbalization
- ✅ DO: Use neutral prompts ("What are you thinking?")
- ✅ DO: Take detailed notes
- ❌ DON'T: Lead or guide the participant
- ❌ DON'T: Answer questions about the interface
- ❌ DON'T: Interrupt their natural workflow

#### Neutral Prompts
- "What are you thinking right now?"
- "Can you tell me what you're looking at?"
- "What do you expect will happen when you click that?"
- "How are you feeling about this task?"

---

## Test Session 1

### Session Metadata
**Date**: December 20, 2025
**Participant ID**: P004
**Demographics**:
- Role: Student
- Age Range: 18-24
- Technical Proficiency: Intermediate
- First-time User: Yes
- Device Used: Desktop
- **Analytics Session ID**: `session_1766227618825_mlgyineya`

**Session Duration**: 13 minutes

### Tasks Assigned

#### Task 1: Login and Dashboard Exploration
**Participant Verbalization**:
> "Okay, signing up now... the neon theme looks pretty cool. The dashboard is clean, I can see my courses right away."

**Facilitator Notes**:
- Time to complete: 45 seconds
- Ease of completion: Easy

#### Task 2: Find and View a Course
**Participant Verbalization**:
> "Going to 'Courses'... I'll join 'Advanced Web Dev'. The join button was easy to find."

**Facilitator Notes**:
- Time to complete: 28 seconds

#### Task 3: Submit an Assignment
**Participant Verbalization**:
> "Where are the assignments? Ah, the button on the left. Okay, 'Project Prototype'. Uploading my PDF now... wait, did it go through? Ah, okay, I see the success message."

**Facilitator Notes**:
- Time to complete: 85 seconds
- Observation: User hesitated for a second after upload before the success alert appeared.

---

## Test Session 2

### Session Metadata
**Date**: December 20, 2025
**Participant ID**: P005
**Role**: Teacher
- **Analytics Session ID**: `session_1766228852242_r55pp3oy6`

**Session Duration**: 22 minutes

### Key Findings:
- Teacher role found the course management intuitive.
- Participant liked the "Live Session" tracking feature.
- Expressed that the "Discussion Board" reversed order (newest first) makes much more sense.

---

## Test Session 3

### Session Metadata
**Date**: December 20, 2025
**Participant ID**: P001 (Pilot)
- **Analytics Session ID**: [Manual Log]

**Session Duration**: 15 minutes

### Key Findings:
- Identified that the "Exam Date Sheet" was too low in the sidebar in early versions.
- Feedback led to the iteration in v1.3 where the button was moved to a more prominent position.

---

## Cross-Session Analysis

### Common Patterns

#### Across All Participants
**Consistently Easy Tasks**:
1. [Task that everyone completed easily]
2. [Task that everyone completed easily]

**Consistently Difficult Tasks**:
1. [Task that multiple users struggled with]
2. [Task that multiple users struggled with]

**Common Confusions**:
1. [Issue mentioned by 2+ participants]
2. [Issue mentioned by 2+ participants]

### Comparative Metrics

| Metric | P001 | P002 | P003 | Average |
|--------|------|------|------|---------|
| Task Success Rate | [X%] | [X%] | [X%] | [X%] |
| Avg Task Time (sec) | [X] | [X] | [X] | [X] |
| SUS Score | [X] | [X] | [X] | [X] |
| Satisfaction (1-5) | [X] | [X] | [X] | [X] |
| Errors | [X] | [X] | [X] | [X] |

---

## Key Findings & Recommendations

### Critical Issues (Priority 1)
1. **Issue**: [Description based on think-aloud findings]
   - **Evidence**: Observed in [X/3] sessions
   - **Impact**: [High/Medium/Low]
   - **Recommended Fix**: [Specific solution]

2. **Issue**: [Description]
   - **Evidence**: [References to sessions]
   - **Impact**: [High/Medium/Low]
   - **Recommended Fix**: [Specific solution]

### Important Issues (Priority 2)
[List medium-priority issues]

### Minor Issues (Priority 3)
[List low-priority issues]

---

## Design Thinking Insights

### Empathy Gains
*What we learned about users*
- [Insight about user needs]
- [Insight about user behavior]
- [Insight about user expectations]

### Problem Redefinition
*How testing changed our understanding*
- [Original assumption] → [New understanding]
- [Original assumption] → [New understanding]

### Iteration Opportunities
*Where to improve in next cycle*
1. [Improvement based on findings]
2. [Improvement based on findings]
3. [Improvement based on findings]

---

## Positive Validations

### Features That Resonated
1. [Feature users loved]
   - Quote: "[User quote]"
   - Why it worked: [Analysis]

2. [Feature users loved]
   - Quote: "[User quote]"
   - Why it worked: [Analysis]

### Design Decisions Validated
- [Decision that testing confirmed was correct]
- [Decision that testing confirmed was correct]

---

## Appendix

### Testing Environment
- **Location**: [Where conducted]
- **Setting**: [Lab/Remote/Field]
- **Tools Used**: 
  - Screen recording: [Yes/No - Tool name]
  - Audio recording: [Yes/No - Tool name]
  - Analytics tracking: Yes (analytics.js)
  - Note-taking: [Method]

### Participant Recruitment
- **Method**: [How participants were recruited]
- **Incentive**: [If any]
- **Selection Criteria**: [Requirements]

### Ethical Considerations
- ✅ Informed consent obtained
- ✅ Right to withdraw explained
- ✅ Data anonymization ensured
- ✅ Recordings stored securely

---

## Session Recording Links
*For internal use only*

- Session 1 (P001): [Link/Path to recording]
- Session 2 (P002): [Link/Path to recording]
- Session 3 (P003): [Link/Path to recording]

---

## Next Steps

### Immediate Actions
1. [ ] Review all session recordings
2. [ ] Synthesize findings into presentation
3. [ ] Create prioritized fix list
4. [ ] Share results with team
5. [ ] Plan next iteration based on insights

### For Final Presentation
- **Highlight**: [Key story to tell from think-aloud testing]
- **Demo**: [Specific user quote or behavior to showcase]
- **Impact**: [How findings will improve the portal]

---

**Document Status**: Template / In Progress / Complete
**Last Updated**: December 19, 2025
**Next Session Scheduled**: [Date/Time]
